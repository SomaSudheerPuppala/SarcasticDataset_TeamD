{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7dfad28-0ab8-4c6d-b805-17a9fe5b9cfd",
   "metadata": {},
   "source": [
    "### Binary Classification with classes combined on the Best Performing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a59b93-9ad7-45c1-b96e-8b238d934aba",
   "metadata": {},
   "source": [
    "## Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "245fcca3-8626-433c-a327-2d63386b50b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import TFRobertaModel\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06edefc-27ac-46b5-a713-1badf9ecce29",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff70515c-46c6-45c0-9f66-1359604325c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loading the entire dataset\n",
    "data = pd.read_csv('train.csv')\n",
    "\n",
    "# Sampling 10% of the data\n",
    "sampled_data = data.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Saving the test data for later use\n",
    "test_data = pd.read_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4799e8a4-d66a-4734-ac6c-dd95b370ad11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       figurative\n",
       "1       figurative\n",
       "2       figurative\n",
       "3       figurative\n",
       "4       figurative\n",
       "           ...    \n",
       "8123       sarcasm\n",
       "8124       sarcasm\n",
       "8125       sarcasm\n",
       "8126       sarcasm\n",
       "8127       sarcasm\n",
       "Name: class, Length: 8128, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        import re\n",
    "        text = re.sub(r'http\\S+|www\\S+|@\\w+|#\\w+|[^A-Za-z0-9\\s]', '', text)\n",
    "        text = text.lower()\n",
    "    else:\n",
    "        text = \"\"\n",
    "    return text\n",
    "\n",
    "# Applying preprocessing\n",
    "sampled_data['tweets'] = sampled_data['tweets'].astype(str).fillna('')\n",
    "sampled_data['clean_tweets'] = sampled_data['tweets'].apply(preprocess_text)\n",
    "test_data['tweets'] = test_data['tweets'].astype(str).fillna('')\n",
    "test_data['clean_tweets'] = test_data['tweets'].apply(preprocess_text)\n",
    "\n",
    "# Ensuring no NaN values in the 'class' column\n",
    "sampled_data['class'] = sampled_data['class'].fillna(-1)\n",
    "test_data['class'] = test_data['class'].fillna(-1)\n",
    "test_data['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c4df5d4-26a7-4407-ae8d-b74c2046e3c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "8123    1\n",
       "8124    1\n",
       "8125    1\n",
       "8126    1\n",
       "8127    1\n",
       "Name: binary_class, Length: 8128, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mapping labels for binary classification\n",
    "sampled_data['binary_class'] = sampled_data['class'].apply(lambda x: 1 if x == 'sarcasm' else 0)\n",
    "test_data['binary_class'] = test_data['class'].apply(lambda x: 1 if x == 'sarcasm' else 0)\n",
    "test_data['binary_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe3bd4a4-daf0-49a0-83ab-aee420341cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Initializing the tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "max_length = 50\n",
    "\n",
    "# Tokenizing the training data\n",
    "X_train_tokens = tokenizer.batch_encode_plus(\n",
    "    sampled_data['clean_tweets'].tolist(),\n",
    "    max_length=max_length,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "# Tokenizing the test data\n",
    "X_test_tokens = tokenizer.batch_encode_plus(\n",
    "    test_data['clean_tweets'].tolist(),\n",
    "    max_length=max_length,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "# Extracting the labels\n",
    "y_train = sampled_data['binary_class'].values\n",
    "y_test = test_data['binary_class'].values\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a5a6755-1faa-4caf-916d-65dee65f49d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['roberta.embeddings.position_ids', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaModel were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "229/229 [==============================] - 1807s 8s/step - loss: 0.5244 - accuracy: 0.7445 - val_loss: 0.4606 - val_accuracy: 0.7755\n",
      "Epoch 2/3\n",
      "229/229 [==============================] - 1735s 8s/step - loss: 0.4461 - accuracy: 0.7723 - val_loss: 0.4456 - val_accuracy: 0.7840\n",
      "Epoch 3/3\n",
      "229/229 [==============================] - 1733s 8s/step - loss: 0.3752 - accuracy: 0.8159 - val_loss: 0.5094 - val_accuracy: 0.7877\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Defining the RoBERTa-based model\n",
    "input_ids = Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n",
    "attention_mask = Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "roberta_model = TFRobertaModel.from_pretrained('roberta-base')\n",
    "roberta_output = roberta_model(input_ids, attention_mask=attention_mask)[0]\n",
    "cls_token = roberta_output[:, 0, :]  # Extract the CLS token\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(cls_token)  # Single unit for binary classification\n",
    "roberta_classifier = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "roberta_classifier.compile(optimizer=Adam(learning_rate=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the model\n",
    "history = roberta_classifier.fit(\n",
    "    {'input_ids': X_train_tokens['input_ids'], 'attention_mask': X_train_tokens['attention_mask']},\n",
    "    y_train,\n",
    "    epochs=3,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bf97f7c-21f0-43c2-9789-ee4b19e7d708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254/254 [==============================] - 635s 2s/step\n",
      "RoBERTa Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     regular       0.81      0.89      0.85      6023\n",
      "     sarcasm       0.56      0.41      0.47      2105\n",
      "\n",
      "    accuracy                           0.76      8128\n",
      "   macro avg       0.69      0.65      0.66      8128\n",
      "weighted avg       0.75      0.76      0.75      8128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Making predictions\n",
    "predictions = roberta_classifier.predict({'input_ids': X_test_tokens['input_ids'], 'attention_mask': X_test_tokens['attention_mask']})\n",
    "\n",
    "# Converting predictions to binary labels\n",
    "y_pred = (predictions > 0.5).astype(int).flatten()  # Flatten to 1D array\n",
    "\n",
    "# Generating the classification report\n",
    "report = classification_report(y_test, y_pred, target_names=['regular', 'sarcasm'])\n",
    "print(\"RoBERTa Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d950870f-b5b9-4da7-8cd4-8e7b620fe4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
